{"name":"ELK-data-analysis","tagline":"","body":"# DATA ANALYSIS\r\nELK stack, with twitter stream input.\r\n\r\n## USE CASE, FOR LEARNING PURPOSES ONLY\r\nStreamed tweets with the top two presidential contenders poll results at a constituency level as it was made public and shared on various social media and on twitter in particular.<br/>\r\nWith the use open-source technologies, aim is to showcase how much insights can be gained from unstructured data, leading to informed decision all around for various purposes.<br/>\r\n**BE REMINDED THAT** : *Care has not been taken to ensure the accuracy, completeness and reliability of the tweets and it`s contained information , we assumes no responsibility therefore.*  \r\n\r\n##Sample tweets \r\n> - Jimbo la Mwanga, Kilimanjaro Kura: 41,136 @MagufuliJP (CCM):  25,738 @edwardlowassatz (Chadema): 15,148 #tanzaniadecides\r\n> - Jimbo Hanang, Manyara Kura: 99,464 @MagufuliJP (CCM):  63,205 @edwardlowassatz (Chadema): 32,367 #tanzaniadecides\r\n> - Jaji Lubuva Matokeo ya Urais Jimbo la Lindi Mjini Kura: 38,992 @MagufuliJP (CCM): 21,088 @edwardlowassatz (Chadema): 17,607 #tanzaniadecides\r\n\r\n# Resulted Dashboard - Kibana\r\n![dashboard](https://github.com/iamiddy/duplicate-eraser/raw/master/dashboard-screenshot.png)\r\n\r\n**DISCLAIMER :**\r\n*Care has not been taken to ensure the accuracy, completeness and reliability of the tweets and it`s contained information , we assumes no responsibility therefore.*\r\n\r\n\r\n###Centralize Data Processing of All Types Ex. TWEETS\r\n>Logstash is a data pipeline that helps you process logs and other event data from a variety of systems. With 200 plugins and counting, Logstash can connect to a variety of sources and stream data at scale to a central analytics system. <br/>\r\n\r\n- In this context used twitter as a logistash datasource and elasticsearch as central data-store for analytics, refer folder **config/** sample configuration looks like\r\n\r\n``` ruby\r\ninput {\r\n  twitter {\r\n      consumer_key => \"<<CONSUMER_KEY>>\"\r\n      consumer_secret => \"<<CONSUMER_SECRET>>\"\r\n      keywords => [\"Kura\", \"Jimbo \"]\r\n      oauth_token => \"<<AUTH_TOKEN>>\"\r\n      oauth_token_secret => \"<<AUTH_TOKEN_SECRET>>\"\r\n      full_tweet => true\r\n   }\r\n}\r\n\r\noutput {\r\n if \"Kata ya\" in [text] and \"Kura:\" in [text] and \"@MagufuliJP (CCM):\" in [text] and \"@edwardlowassatz (Chadema):\" in [text]{\r\n     stdout { codec => rubydebug }\r\n    elasticsearch {\r\n\t    protocol => \"http\"\r\n\t    host => \"localhost\"\r\n\t    index => \"twitter\"\r\n\t    document_type => \"tweet\"\r\n  }\r\n }\r\n\r\n}\r\nrun command : bin/logstash -f Tanzania-uchaguzi-2015.conf\r\n```\r\n\r\n### DEALING WITH DUPLICATES\r\n- Re-tweets etc\r\n``` java\r\npublic class DuplicatesFindUtility {\r\n\r\n    public static List<String> findDuplicateResultsIds(Iterable<ConstituencyResult> resultIterable){\r\n        List<String> duplicateResultsId = new ArrayList<>();\r\n        Map<String, List<ConstituencyEntity>> resultGroups = StreamSupport\r\n                .stream(resultIterable.spliterator(), false)\r\n                .map(ConstituencyEntity::new)\r\n                .collect(Collectors.groupingBy(ConstituencyEntity::getName));\r\n\r\n        resultGroups.forEach((n, l) -> {\r\n            if (l.size() > 1) {\r\n                System.out.println(n + \" --- X \" + l.size() );\r\n                for (int i = 1; i < l.size(); i++) {\r\n                    duplicateResultsId.add(l.get(i).getId());\r\n\r\n                };\r\n            }\r\n        });\r\n\r\n        return duplicateResultsId;\r\n    }\r\n\r\n    private static void deleteDuplicates(String index, String type, List<String> ids){\r\n        Client client = new TransportClient().addTransportAddress(new InetSocketTransportAddress(\"localhost\", 9300));\r\n        deleteDuplicateConstituencesByIds(client,index,type,ids);\r\n    }\r\n\r\n    public static void deleteDuplicateConstituencesByIds(Client client, String index, String type, List<String> iDsToDelete){\r\n        iDsToDelete.forEach(id ->{\r\n            DeleteResponse deleteResponse = client.prepareDelete(index,type,id)\r\n                    .execute()\r\n                    .actionGet();\r\n            System.out.println(id +\" >>>>> \" +deleteResponse.isFound());\r\n        });\r\n\r\n    }\r\n\r\n    public static void  findAndDeleteDuplicateResultsIds(Iterable<ConstituencyResult> resultIterable){\r\n        System.out.println(\"1 : Identifying duplicates.......\");\r\n        List<String> duplicatesIds = findDuplicateResultsIds(resultIterable);\r\n        System.out.println(\"2 : found \" + duplicatesIds.size() + \" duplicates \");\r\n        if(duplicatesIds.size() > 0){\r\n            System.out.println(\"3: Deleting duplicates please wait ......\");\r\n            deleteDuplicates(\"constituencywitter\",\"constituencytweet\",duplicatesIds);\r\n            System.out.println(\"5: CleanUp completed successfully\");\r\n        }\r\n\r\n    }\r\n\r\n}\r\n```\r\n\r\n#References\r\n- [Grok Debugger](https://grokdebug.herokuapp.com/)\r\n- [Logistash](https://www.elastic.co/products/logstash)\r\n- [Elasticsearch](https://www.elastic.co/products/elasticsearch)\r\n- [Kibana](https://www.elastic.co/products/kibana)\r\n- [Elasticsearch Java API](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/index.html)\r\n- [Spring Data Elasticsearch](https://github.com/spring-projects/spring-data-elasticsearch)\r\n- [Indexing Twitter With Logstash and Elasticsearch](http://david.pilato.fr/blog/2015/06/01/indexing-twitter-with-logstash-and-elasticsearch/)\r\n\r\n[CEITECHS](http://www.ceitechs.com) is here to  help you Manage vast amount of data and gain valuable insights you've not imagined ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}